#+LAYOUT: docs-manual
#+TITLE: Snowflake
#+SUMMARY:
#+hugo_base_dir: ../../
#+hugo_section: tools
#+hugo_custom_front_matter: :toc true :summary "A modern data engineering platform."
#+hugo_custom_front_matter: :chapter true
#+hugo_custom_front_matter: :aliases '("/snow" "/sf" "/snowflake" "/sn")
#+hugo_custom_front_matter: :warning "THIS FILE WAS GENERATED BY OX-HUGO, DO NOT EDIT!!!"
#+PROPERTY: header-args:python :exports both :eval yes :results value scalar
#+hugo_level_offset: 0
#+STARTUP: showall

* Snowflake

*Snowflake* is a cloud native database that provides a wealth of
analytical and data mining features for processing, integrating, and
presenting data. *Data platform* could be used to describe Snowflake, as
it offers features traditionally found in data warehouses, lakes, and
streaming-processing platforms like Kafka.

#+hugo: more


* Why Snowflake?
:PROPERTIES:
:ID:       5ce4639d-0017-45d7-a564-d8625c516fc5
:END:

The shortcomings of traditional data analytics environments have been
addressed with Snowflake's ease of storage, retrieval, and analysis of
large quantities of client data.

*Topics:*

- Fully managed: no hardware or server provisioning required
- Compute / storage decoupled
- Auto-scaling, auto-suspend, etc.
- Support for semi-structured data (JSON, VARIANT, etc.)
- Zero-copy cloning, time travel, data sharing
- Strong ecosystem and connector support

* Key Concepts & Architecture
:PROPERTIES:
:ID:       2238eb3b-bd76-4f84-8c95-e4f7e8a206af
:END:

*Topics:*

- Snowsight[fn:3] UI (web interface)
  - Worksheets: old UI, now *Workspaces* (UI: Projects => Workspaces).
  - Workspaces can be synchronized with Git.
- SnowSQL CLI (Snowflake command-line utility)
- Notebooks (Snowflake Notebooks)
- Architecture: hybrid of shared-disk and shared-nothing.
  - Central storage + multiple MPP compute clusters.
- Snowflake Documentation
- Cloud Platforms: Runs on AWS, Azure, or GCP.
- Snowflake Documentation
- Micro-partitions (internal detail)
- Compute / Storage separation
- Zero-copy cloning, Time Travel, Fail-safe

** Multi-Cluster Shared Data Architecture

Typical distributed architectures like shared-disk or shared-nothing
keep independent copies of data locally, which are synchronized, or
kept at a single point for shared-disk. Shared-disk has the downside
of a fragile single point of failure, where shared-noting is expensive
to keep synchronized and easy to over-provision. Snowflake takes a
different approach by segregating the system into layers, called
/"Multi-cluster Shared Data Architecture"/:

1. *Data Storage*
2. *Query Processing* (Virtual Warehouses)
3. *Cloud Services*

This separation allows each layer to scale entirely independently.

*** Data Storage Layer

Snowflake data is stored in a *column-oriented, partitioned, encrypted
format* highly optimized for the blob storage it is written to.

By default, strong *AES-256* encryption is applied to data written to
the backing blob storage. Snowflake inherits the durability and
availability guarantees provided by their backing services - in the
case of Snowflake's proprietary columnar storage format[fn:1], AWS S3
blob storage.

Snowflake divides written files into *micro-partitions* so only columns
that must be read or written are loaded during a query.

Table data is billed at a flat rate per month, and only accessible via
Snowflake queries.

*** Query Processing Layer

*Virtual Warehouses* in the query processing layer cache table data
required for queries locally, while leaving the majority of data in
storage. Queries are executed in these warehouses, which are EC2[fn:2]
instances provisioned by Snowflake in an ephemeral manner.

*Small - 6XL* warehouse sizes (t-shirt sizes) are available.

Even with many warehouses operating on the data, Snowflake uses an
[[https://www.mongodb.com/resources/products/capabilities/acid-compliance][ACID]] compliant global layer (the *transaction manager*)to ensure the
data from each transaction is immediately available to all warehouses.

*** Global Services Layer

Highly available system management services common to all Snowflake
users, responsible for optimizing queries, scaling and managing
infrastructure, metadata caching, authentication, and security.

Snowflake is a global multi-tenancy service.

** Editions & Pricing
:PROPERTIES:
:ID:       9706ae40-c402-495c-8333-40c841bf2f99
:END:

*Topics:*

- Editions: Standard, Enterprise, Business Critical, Virtual Private Snowflake (VPS)
- Pricing model: credits for compute + storage + usage
- How edition differences affect features (e.g. multi-cluster, data protection)

** Editions

1. *Standard*
2. *Enterprise* adds database failover, multi-cluster warehouses, and
   additional data protection and encryption features.
3. *Business Critical*
4. *Virtual Private* is isolated from the global Snowflake program.

** Billing

*On demand* and *capacity* models are available. Capacity rewards upfront
payment.

*Costs:*

1. [[https://docs.snowflake.com/en/user-guide/cost-understanding-compute#what-are-credits][Snowflake Credits]]:
   - Virtual Warehouse Services
     - Billed per second, minimum 60 seconds, based on size.
   - Cloud Services
     - Metadata operations that don't require a warehouse
     - Burns 4.4 credits per compute-hour
     - [[https://docs.snowflake.com/en/user-guide/cost-understanding-compute#understanding-billing-for-cloud-services-usage][Cloud services adjustment]]: only billed if all services exceed
       10% of the daily virtual warehouse credits used.
   - Serverless Services
     - Each has its own rate
2. Dollars & Cents:
   - [[https://docs.snowflake.com/en/user-guide/cost-understanding-data-storage][Storage]]: tables, stages, time travel data
     - Charged at a /flat rate per TB/
   - Data Transfer & Egress
     - Transfer charges between regions (~COPY INTO~)
     - Replicating data between regions

* Integration and Connectors

*Topics*:

- Snowsight, Snowpark, Drivers, CLI
- JDBC, ODBC, Python connector
- Spark / Snowflake connector
- Kafka Connector
- BI tools (Tableau, Power BI, etc.)
- Data marketplace & data sharing

A variety of methods exist to interact with Snowflake's platform.

** Snowsight

*Snowsight* is the web interface provided by Snowflake. It is
continuously improved.

** Snowflake Drivers & Connectors

*Snowflake Drivers/Connectors* refer to programmatic APIs to interact with
Snowflake from your favourite programming language. The connector for
[[https://docs.snowflake.com/en/developer-guide/python-connector/python-connector][python]] enables all typical operations, in addition to reading and
writing [[https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-pandas][pandas dataframes]]. *Cursors* can be used to connect and execute
SQL statements.

** Snowflake CLI

*Snowflake CLI* can be installed to connect to Snowflake via the command
line. The legacy client, [[https://docs.snowflake.com/en/user-guide/snowsql][snowsql]], is now /out of date/.

** Partner Tools

*Partner Tools* enable connection to your account via /SSO/ to read and
analyze your data. BI, data integration, security, and governance are
common use cases.

** Snowpark

*Snowpark* refers to programmatic APIs to run heavy data manipulation
within Snowflake warehouses, leaving the data /within Snowflake/ during
processing.

See the [[https://docs.snowflake.com/en/developer-guide/snowpark/python/index][Snowpark Developer Guide for Python]]. I typically add a
configuration file in ~.snowflake/config.toml~ with the following
content:

#+begin_src toml
default_connection_name = "my_main_account"

[connections.my_main_account]
account = "myaccount"
user = "jdoe"
password = "******"
warehouse = "my-wh"
database = "my_db"
schema = "my_schema"

[cli.logs]
save_logs = true
level = "info"
path = "/home/<you>/.snowflake/logs"
#+end_src

* Snowflake Objects & DDL Commands
:PROPERTIES:
:ID:       27495aa5-d586-461e-ab3c-65c9465f00c8
:END:

*Topics:*

- Databases, Schemas, Tables, Views
- External Tables, Streams, Tasks
- Sequences, Stages
- Examples of CREATE / ALTER / DROP
- Cloning & object versioning
- DDL = /data definition language/

*Objects* in Snowflake allow nearly all aspects of the data platform to
be configured with unique access and usage restrictions, from the
*Organization* level down to tables and views.

*Account Objects:*

- Network Policy
- User
- Role
- Database => Schema
- Warehouse
- Share
- Resource Monitor

*Schema Objects:*

- Stage
- Pipe
- Procedure
- Function
- Table
- View
- Task
- Stream

To work with objects within a schema you must set the *context* for the
following operations with this set of commands:

#+begin_src sql
USE ROLE <role>;
USE WAREHOUSE <warehouse>;
USE DATABASE <database>;
USE SCHEMA <schema>;
#+end_src

** Object Naming Rules

The name of a database, schema, or table must be unique and start with
'~A-Z~', and are not case sensitive unless encased in double quotes.
Special characters can only be used within quotes.

** General DDL Commands

Data Definition Language (DDL) commands are used to manipulate objects
in Snowflake, including setting parameters on account and session objects.

Generally these are available:

- [[https://docs.snowflake.com/en/sql-reference/sql/use][USE]] <object>
- [[https://docs.snowflake.com/en/sql-reference/sql/create][CREATE]] <object>
- [[https://docs.snowflake.com/en/sql-reference/sql/alter][ALTER]] <object>
- [[https://docs.snowflake.com/en/sql-reference/sql/create-or-alter][CREATE OR ALTER]] <object>
- [[https://docs.snowflake.com/en/sql-reference/sql/drop][DROP]] <object>
- [[https://docs.snowflake.com/en/sql-reference/sql/show][SHOW]] <objects>
- [[https://docs.snowflake.com/en/sql-reference/sql/desc][DESCRIBE]] <object>
- [[https://docs.snowflake.com/en/sql-reference/sql/comment][COMMENT]]

To get the definition for an object, use the following query:

#+begin_src sql
SELECT get_ddl('<type>', '<NAME>');

-- For example:
SELECT get_ddl('table', 'CUSTOMERS');
#+end_src

By default, Snowflake will use all your permissions (~SECONDARY_ROLES
= ALL~) to provide the DDL, revealing all secure features.

** Databases

A database is associated with one account. See [[https://docs.snowflake.com/en/sql-reference/commands-database#database][docs]].

#+begin_src sql
CREATE DATABASE MY_DATABASE;

-- Cloned
CREATE DATABASE CLONED_DB CLONE MY_DATABASE;

-- Replica
CREATE DATABASE REPLICA_DB AS REPLICA OF MY_DATABASE
  DATA_RETENTION_TIME_IN_DAYS = 3;

-- From share object provided by external account
CREATE DATABASE SHARED_DB FROM SHARE S9DF89.SHARE;

SHOW DATABASES;
#+end_src

** Schemas

A schema is associated with one database. See [[https://docs.snowflake.com/en/sql-reference/commands-database#schema][docs]].

#+begin_src sql
USE DATABASE MY_DATABASE;
CREATE SCHEMA MY_SCHEMA;

-- Cloned
CREATE SCHEMA CLONED_SCM CLONE MY_DATABASE.MY_SCHEMA;

SHOW SCHEMAS;
SHOW SCHEMAS LIKE 'TPCH%';
#+end_src

** Tables

A table is associated with one schema. See [[https://docs.snowflake.com/en/sql-reference/commands-table#table][docs]].

- Tables are *permanent by default*, but can also be:
  - Temporary (for just the current Snowsight[fn:3] session)
  - Transient (no failsafes)
  - External (file-based tables stored outside Snowflake)
    - You can specify the path and type for these.
- Standard accounts can set *time travel* on permanent tables to a day,
  and enterprise accounts can be set up to 90 days, which enables
  un-dropping the table and restoring from a particular timestamp.

To see if a table is external, and its properties, you can use SHOW:

#+begin_src sql
SHOW TABLES;
SHOW TABLES LIKE '<table name>';
#+end_src

** Views

- Views don't contribute to storage cost.
- Can be used to reveal a subset of table data.
- *Materialized* views are periodically refreshed and stores the results
  of the query independently from the source table.
- *Secure* views are only visible to authorized users.
- Querying views that rely on dropped tables will throw an error.

#+begin_src sql
SHOW VIEWS;
#+end_src

* Data Loading & Unloading
:PROPERTIES:
:ID:       4d1dcea7-9c22-4c51-a70f-b6ee3dfb09b7
:END:

*Topics:*

- Stages: internal (user, table, named) vs external cloud stages
- Snowflake Documentation
- COPY INTO / bulk load
- Snowpipe (continuous ingestion)
- Snowflake Documentation
- Snowpipe Streaming
- Loading semi-structured data and schema inference

* Querying & Data Manipulation Language
:PROPERTIES:
:ID:       a41532df-4d4c-460a-b6a0-692d2810513f
:END:

Data Manipulation Language (DML) refers to the normal SQL methods of
CRUDlike updates to data, with some special Snowflake nuances.

*Topics:*

- SELECT, INSERT, UPDATE, DELETE, MERGE
- Working with semi-structured data (VARIANT, OBJECT, ARRAY)
- Window functions, CTEs, analytic SQL
- Transforming data in Snowflake

** DECLARE & Snowflake Scripting

Snowflake SQL has support for procedural logic and error handling
using a built-in SQL extension called [[https://docs.snowflake.com/en/developer-guide/snowflake-scripting/index][Snowflake scripting]]. You can
declare variables and cursors, use control flow logic, handle
exceptions, and update tables.

#+begin_src sql
DECLARE
  -- Variables, cursors.
BEGIN
  -- SQL statements
EXCEPTION
  -- Error handling
END;
#+end_src

This example from the [[https://docs.snowflake.com/en/developer-guide/snowflake-scripting/use-cases#update-table-data-with-user-input][use cases]] demonstrates variable assignment and a
for loop:

#+begin_src sql
-- docs.snowflake.com/en/developer-guide/snowflake-scripting/use-cases

CREATE OR REPLACE PROCEDURE apply_bonus(bonus_percentage INT, performance_value INT)
  RETURNS TEXT
  LANGUAGE SQL
AS

DECLARE
  -- Use input to calculate the bonus percentage
  updated_bonus_percentage NUMBER(2,2) DEFAULT (:bonus_percentage/100);
  --  Declare a result set
  rs RESULTSET;

BEGIN
  -- Assign a query to the result set and execute the query
  rs := (SELECT * FROM bonuses);
  -- Use a FOR loop to iterate over the records in the result set
  FOR record IN rs DO
    -- Assign variable values using values in the current record
    LET emp_id_value INT := record.emp_id;
    LET performance_rating_value INT := record.performance_rating;
    LET salary_value NUMBER(12, 2) := record.salary;
    -- Determine whether the performance rating in the record matches the user input
    IF (performance_rating_value = :performance_value) THEN
      -- If the condition is met, update the bonuses table using the calculated bonus percentage
      UPDATE bonuses SET bonus = ( :salary_value * :updated_bonus_percentage )
        WHERE emp_id = :emp_id_value;
    END IF;
  END FOR;
  -- Return text when the stored procedure completes
  RETURN 'Update applied';
END;
#+end_src

- Cursors
- Resultsets

* Functions & Procedures

*Topics:*

- Stored procedures in SQL, JavaScript, Python
- UDFs in SQL, JS, Python, Java, and Scala
- When, why, how, and the results of these calls
- What is Snowpark[fn:4], how does it work?

** Stored Procedures (SPs)

*Stored procedures* are named collections of SQL statements. In
Snowflake, these can be created with Snowflake Scripting (SQL) but
also JavaScript and via Snowpark[fn:4].

SPs cannot be called in SQL statements, but /can/ make use of the
Snowpark API. *The primary goal is to cause side effects in the system.*

*** JavaScript Stored Procedures

There exists a [[https://docs.snowflake.com/en/developer-guide/stored-procedure/stored-procedures-api][JavaScript Stored Procedures API]] that provides a
~snowflake~ object for use within stored procedures written with JS,
enabling branching, looping, error handling, and the dynamic creation
of SQL statements.

** User Defined Functions (UDFs)

UDFs are custom functions that can be written in SQL, JS, Python,
Java, or Scala. They can accept parameters and return scalar or
tabular results, can be called from SQL statements, and can be
overloaded. Data is converted to supported types as it is passed to
the functions.

UDFs can be called as part of a SQL statement, returning values for
use, but cannot utilize the Snowpark API or libraries. *The primary
goal is complex data processing.*

#+begin_src sql
CREATE FUNCTION JS_SQUARE_ROOT(D double)
  RETURNS DOUBLE
  LANGUAGE JAVASCRIPT
  AS
  $$
  return(Math.sqrt(D));
  $$;

-- Call the function
SELECT js_square_root(2);

-- ==> 1.414213562

-- Drop the function
DROP FUNCTION JS_SQUARE_ROOT(DOUBLE);
#+end_src

- UDFs & SPs can call [[https://docs.snowflake.com/en/developer-guide/external-network-access/external-network-access-overview][external network services]] with configuration.
- [[https://docs.snowflake.com/en/sql-reference/external-functions-introduction][External functions]] can be set to POST a REST gateway on
  AWS/GCP/Azure, but they are slower, less secure, cannot be shared,
  and are scalar only.
- *Overloading:* Functions can have the same name as long as the
  function signature is different.

*** Java Functions

See [[https://docs.snowflake.com/en/developer-guide/udf/java/udf-java-introduction][Snowflake Docs: Java UDFs]].

*Java* UDFs take ~HANDLER~ and ~TARGET_PATH~ parameters - allowing you to
optionally provide a JAR file with classes and functions to use.

#+begin_src sql
CREATE OR REPLACE FUNCTION inline_hello(name STRING)
RETURNS STRING
LANGUAGE JAVA
AS
$$
    if (name == null) {
        return "Hello, you! Inline function called!";
    }
    return "Hello, " + name + "! Inline function called!";
$$;

-- Call the function:
SELECT inline_hello('Ryan');
#+end_src

Or as a pre-compiled JAR:

#+begin_src java
package com.example;

public class HelloUDF {
    public static String hello(String name) {
        if (name == null) {
            return "Hello, you!";
        }
        return "Hello, " + name + "!";
    }
}
#+end_src

Compile to a JAR file:

#+begin_src bash
#! /bin/bash
javac -d . HelloUDF.java
jar cf hello-udf.jar com/example/HelloUDF.class
echo "Assembled JAR for Java UDF"
#+end_src

=> hello-udf.jar

In snowflake, refer to this function in the JAR file:

#+begin_src sql
CREATE OR REPLACE STAGE my_stage;
PUT file://hello-udf.jar @my_stage auto_compress=false;

-- Define the function
CREATE OR REPLACE FUNCTION hello(name STRING)
  RETURNS STRING
  LANGUAGE JAVA
  IMPORTS = ('@my_stage/hello-udf.jar')
  HANDLER = 'com.example.HelloUDF.hello'
  AS
  $$
    /* Inline Java could be placed here that utilizes functions from the JAR. */
  $$;

-- Call the function:
SELECT hello();
#+end_src

==> /[[https://www.youtube.com/watch?v=GO-nsnnmqHA][Hello, you!]]/

*** External Functions

An ~EXTERNAL~ function is a lambda or web service behind a proxy.

#+begin_src sql
CREATE OR REPLACE EXTERNAL FUNCTION blorgon_process(str_input varchar)
  RETURNS variant
  API_INTEGRATION = blorgo9t_08 -- API integration object
  AS 'https://blorgo9t.execute-api.us-west-2.amazonaws.com/prod/blorg'; -- Proxy URL
#+end_src

* Warehouses & Compute
:PROPERTIES:
:ID:       16b1100d-c08c-444c-ae93-a8bf16f629fb
:END:

*Topics:*

- What is a warehouse; required for queries / DML
- Snowflake Documentation
- Warehouse sizes and billing
- Snowflake Documentation
- Auto-suspend, auto-resume
- Multi-cluster warehouses
- Best practices for compute sizing & concurrency

** Performance & Cost Optimization
:PROPERTIES:
:ID:       bc077855-d82a-4d0c-a222-e1bc58bf5824
:END:

*Topics:*

- Choosing warehouse sizes vs concurrency
- Pruning / clustering / clustering keys
- Caching, result caching
- Query profiling / query history
- Avoiding overprovisioning / idle compute

* Security & Access Control
:PROPERTIES:
:ID:       6311ae74-372b-461d-8e05-375e489e2e5b
:END:

*Topics:*

- Account access
- Roles, users, grants, privileges
- Snowflake Documentation
- Object-level permissions
- Row access policies, masking policies
- Network / IP policies, secure views, encryption

Each *account* is created on specific *provider*, in a particular *region*,
as a single Snowflake *edition*. An *organization* can manage one or more
accounts for different departments, projects, or locations.

The ~GLOBALORGADMIN~ account can be used to create more accounts and
manage their lifecycle;

#+begin_src sql
USE ROLE GLOBALORGADMIN;

CREATE ACCOUNT analytics4
  ADMIN_NAME = admin7
  REGION = aws_us_west_2
  etc;

SHOW ACCOUNTS;
#+end_src

The short-form of the organization and account is shown in your URL.

#+begin_src
https://uslkjpw-sjl18827.snowflakecomputing.com/
        ------- --------
          Org.    Acct.
#+end_src

This URL will prompt you to login, then redirect you to *Snowsight*[fn:3].

* Advanced Features
:PROPERTIES:
:ID:       5311cf05-ba3c-45ae-8ac5-cdf8f8146d4c
:END:

*Topics:*

- Time Travel & Fail-safe
- Zero-copy cloning
- Materialized Views
- Tasks & scheduled pipelines
- Search optimization service
- Streams & Change Data Capture (CDC)

** Tasks

- Schedule the execution of a SQL statement or stored procedure.
-

#+begin_src sql
CREATE TASK SYNC1
  WAREHOUSE = WH1
  SCHEDULE = '30 MINUTE' -- can be a CRON expression
AS
  COPY INTO BIG_TABLE_1
  FROM $SOME_STAGE;

-- Start Task Schedule:
-- Requires 'execute task' permission
ALTER TASK SYNC1 RESUME;

-- Stop Task Schedule:
ALTER TASK SYNC1 SUSPEND;
#+end_src

*** Task Graphs (DAGs)

- A root task can be defined with a schedule
- Child tasks are defined /without a schedule/, and can have multiple
  parent dependencies to wait for before executing.

** Streams

- Objects created to view and track DML changes to a source table.
- A stream is queryable, and is created on top of a table.

#+begin_src sql
CREATE STREAM USER_STREAM ON TABLE USERS;
SELECT * FROM USER_STREAM;
#+end_src

These have three additional columns:

- The action (INSERT, UPDATE, DELETE)
- Whether or not it was a write/update operation
- The unique Snowflake row ID

Tasks can be configured to process the stream data:

#+begin_src sql
CREATE TASK NEWUSERS1
  WAREHOUSE = WH1
  SCHEDULE = '5 MINUTE' -- can be a CRON expression
WHEN
  SYSTEM$STREAM_HAS_DATA('USER_STREAM')
AS
  INSERT INTO SEND_EMAIL(ID, NAME) SELECT ID, NAME FROM USER_STREAM;
#+end_src

* Tips, Best Practices & Gotchas
:PROPERTIES:
:ID:       1e1ad07a-59d2-4e30-b025-8ddf3505844b
:END:

*Topics:*

- Avoid leaving warehouses running unnecessarily
- For small queries, very large warehouses may not help
- Use clustering only when needed
- Be aware of billing granularity (per second)
- Watch out for too many small files in load
- Query patterns that can disrupt performance

* COF-C02 - Snowpro Core Certification
:LOGBOOK:
- State "CANCELLED"  from              [2025-12-16 Tue 21:41]
:END:

You can download the study guide for this exam on the [[https://learn.snowflake.com/en/certifications/snowpro-core][Snowflake
COF-C02 Exam Guide]] page. This guide is updated frequently, so go and
request your own copy if possible. It is a 100-question, 115-minute
test. You will be expected to have knowledge of (taken from the
guide):

1. Data loading and transformation in Snowflake
2. Virtual Warehouses - best practices, performance, concurrency
3. DDL and DML queries
4. Working with semi-structured and unstructured data
5. Cloning and time travel
6. Data sharing
7. Account structure and management

The *cole's notes* on each of the key topics are below.

** (24%) Snowflake AI Data Cloud - Features and Architecture

*[GENAI]* Key Concepts:

- *Multi-Cluster Shared Data Architecture*: Understand the separation of *Storage* (centralized on cloud providers like S3/Azure Blob), *Compute* (independent virtual warehouses), and *Cloud Services* (manages metadata, security, and optimization).
- *Snowflake Editions & Capabilities*: Know the differences between *Standard, Enterprise, Business Critical, and VPS*, specifically regarding Time Travel retention (1 day vs. 90 days) and security features like Customer Managed Keys (Tri-Secret Secure).
- *Storage Management*: Snowflake uses *micro-partitions* (immutable, columnar storage) and *data clustering* to optimize query performance without manual indexing.


** (18%) Data Transformations

*[GENAI]* Key Concepts:

- *ELT (Extract, Load, Transform)*: Snowflake prioritizes an ELT approach where raw data is loaded first and then transformed using Snowflake’s compute power via SQL, Tasks, and Streams.
- *Semi-Structured Data Handling*: Master the *VARIANT* data type and the use of *FLATTEN* and lateral joins to query and transform JSON, Avro, and Parquet data.
- *Automated Transformation Tools*: Understand the roles of *Dynamic Tables* (declarative data pipelines), *Streams* (change data capture), and *Tasks* (scheduled SQL execution).


** (18%) Accounts - Access & Security

*[GENAI]* Key Concepts:

- *Role-Based Access Control (RBAC)*: Grasp the hierarchy of system roles (*ACCOUNTADMIN, SECURITYADMIN, USERADMIN, SYSADMIN, PUBLIC*) and the concept of "inheritance" where higher-level roles inherit privileges from lower ones.
- *Authentication & Connectivity*: Familiarize yourself with Multi-Factor Authentication (MFA), *Key-Pair Authentication* for service accounts, and Network Policies for IP whitelisting.
- *Data Governance*: Know the features of *Snowflake Horizon*, including Column-level Security (Dynamic Data Masking) and Row-level Security (Row Access Policies).


** (16%) Performance & Cost Optimization

*[GENAI]* Key Concepts:

- *Virtual Warehouse Scaling*: Differentiate between *Scaling Up* (increasing warehouse size for large, complex queries) and *Scaling Out* (adding clusters to a Multi-Cluster Warehouse to handle high concurrency).
- *Caching Layers*: Understand the three types of cache: *Result Cache* (24-hour persistence), *Local Disk Cache* (data cached on SSDs of the warehouse), and *Cloud Services Cache* (metadata cache).
- *Cost Management Tools*: Learn how to use *Resource Monitors* to set credit limits and the *Account Usage* views to track consumption of compute and storage.


** (12%) Data Loading & Unloading

*[GENAI]* Key Concepts:

- *Bulk vs. Continuous Loading*: Know when to use the *COPY INTO* command (batch loading using virtual warehouses) versus *Snowpipe* (automated, serverless continuous loading).
- *Staging Areas*: Distinguish between *Internal Stages* (User, Table, and Named stages) and *External Stages* (pointing to S3, Azure, or GCS buckets).
- *File Formats & Optimization*: Understand how to define *File Format objects* and why it is a best practice to split large files into 100-250 MB chunks for parallel processing during load.


** (12%) Data Protection and Sharing

*[GENAI]* Key Concepts:

- *Time Travel & Fail-safe*: Remember that *Time Travel* allows querying/restoring data within the retention period (1-90 days), while *Fail-safe* provides a non-configurable 7-day recovery window for Snowflake support only.
- *Zero-Copy Cloning*: Understand that cloning creates a snapshot of data (tables, schemas, or databases) that initially *shares the same storage* as the original, meaning no additional storage costs until the clone is modified.
- *Secure Data Sharing*: Know that *Shares* allow providers to grant read-only access to consumers without copying data, and *Reader Accounts* are used to share data with parties who do not have their own Snowflake account.


* References & Further Reading
:PROPERTIES:
:ID:       ad44f0cb-efbf-44d8-bd07-c53538951428
:END:



Snowflake has tons of interesting documents in their [[https://www.snowflake.com/en/resources/][resource library]],
including migration advice, using Snowflake as a backing database for
agents, and much more.

*Topics:*

- Official Snowflake documentation (docs.snowflake.com)
- Snowflake Documentation
- Tutorials (“Snowflake in 20 minutes”)
- Snowflake Documentation
- Quickstarts & hands-on labs
- Snowflake Quickstarts
- SQL command reference
- Snowflake Documentation

* Footnotes

[fn:4] [[https://www.snowflake.com/en/product/features/snowpark/][Snowpark]] is a multi-language framework for executing remote
data operations within Snowflake warehouses, close to the data.

[fn:3] *Snowsight* is the [[https://docs.snowflake.com/en/user-guide/ui-snowsight][Snowflake web interface]].

[fn:2] *EC2* or equivalent cloud-based virtual machines.

[fn:1] *Columnar Storage* is read-optimized, enabling quick seeking
through rows without having to read over the entire content of each
row, like a CSV or other row-oriented storage format.
