+++
title = "Snowflake"
author = ["Ryan Fleck"]
draft = false
toc = true
summary = "A modern data engineering platform."
chapter = true
aliases = ["/snow", "/sf", "/snowflake", "/sn"]
warning = "THIS FILE WAS GENERATED BY OX-HUGO, DO NOT EDIT!!!"
+++

# Snowflake {#snowflake}

**Snowflake** is a cloud native database that provides a wealth of
analytical and data mining features for processing, integrating, and
presenting data. **Data platform** could be used to describe Snowflake, as
it offers features traditionally found in data warehouses, lakes, and
streaming-processing platforms like Kafka.

<!--more-->


## COF-C02 - Snowpro Core Certification {#cof-c02-snowpro-core-certification}

You can download the study guide for this exam on the [Snowflake
COF-C02 Exam Guide](https://learn.snowflake.com/en/certifications/snowpro-core) page. This guide is updated frequently, so go and
request your own copy if possible. It is a 100-question, 115-minute test.


# Why Snowflake? {#why-snowflake}

The shortcomings of traditional data analytics environments have been
addressed with Snowflake's ease of storage, retrieval, and analysis of
large quantities of client data.

**Topics:**

-   Fully managed: no hardware or server provisioning required
-   Compute / storage decoupled
-   Auto-scaling, auto-suspend, etc.
-   Support for semi-structured data (JSON, VARIANT, etc.)
-   Zero-copy cloning, time travel, data sharing
-   Strong ecosystem and connector support


# Key Concepts &amp; Architecture {#key-concepts-and-architecture}

**Topics:**

-   Snowsight[^fn:1] UI (web interface)
    -   Worksheets: old UI, now **Workspaces** (UI: Projects =&gt; Workspaces).
    -   Workspaces can be synchronized with Git.
-   SnowSQL CLI (Snowflake command-line utility)
-   Notebooks (Snowflake Notebooks)
-   Architecture: hybrid of shared-disk and shared-nothing.
    -   Central storage + multiple MPP compute clusters.
-   Snowflake Documentation
-   Cloud Platforms: Runs on AWS, Azure, or GCP.
-   Snowflake Documentation
-   Micro-partitions (internal detail)
-   Compute / Storage separation
-   Zero-copy cloning, Time Travel, Fail-safe


## Multi-Cluster Shared Data Architecture {#multi-cluster-shared-data-architecture}

Typical distributed architectures like shared-disk or shared-nothing
keep independent copies of data locally, which are synchronized, or
kept at a single point for shared-disk. Shared-disk has the downside
of a fragile single point of failure, where shared-noting is expensive
to keep synchronized and easy to over-provision. Snowflake takes a
different approach by segregating the system into layers, called
_"Multi-cluster Shared Data Architecture"_:

1.  **Data Storage**
2.  **Query Processing** (Virtual Warehouses)
3.  **Cloud Services**

This separation allows each layer to scale entirely independently.


### Data Storage Layer {#data-storage-layer}

Snowflake data is stored in a **column-oriented, partitioned, encrypted
format** highly optimized for the blob storage it is written to.

By default, strong **AES-256** encryption is applied to data written to
the backing blob storage. Snowflake inherits the durability and
availability guarantees provided by their backing services - in the
case of Snowflake's proprietary columnar storage format[^fn:2], AWS S3
blob storage.

Snowflake divides written files into **micro-partitions** so only columns
that must be read or written are loaded during a query.

Table data is billed at a flat rate per month, and only accessible via
Snowflake queries.


### Query Processing Layer {#query-processing-layer}

**Virtual Warehouses** in the query processing layer cache table data
required for queries locally, while leaving the majority of data in
storage. Queries are executed in these warehouses, which are EC2[^fn:3]
instances provisioned by Snowflake in an ephemeral manner.

**Small - 6XL** warehouse sizes (t-shirt sizes) are available.

Even with many warehouses operating on the data, Snowflake uses an
[ACID](https://www.mongodb.com/resources/products/capabilities/acid-compliance) compliant global layer (the **transaction manager**)to ensure the
data from each transaction is immediately available to all warehouses.


### Global Services Layer {#global-services-layer}

Highly available system management services common to all Snowflake
users, responsible for optimizing queries, scaling and managing
infrastructure, metadata caching, authentication, and security.

Snowflake is a global multi-tenancy service.


## Editions &amp; Pricing {#editions-and-pricing}

**Topics:**

-   Editions: Standard, Enterprise, Business Critical, Virtual Private Snowflake (VPS)
-   Pricing model: credits for compute + storage + usage
-   How edition differences affect features (e.g. multi-cluster, data protection)


## Editions {#editions}

1.  **Standard**
2.  **Enterprise** adds database failover, multi-cluster warehouses, and
    additional data protection and encryption features.
3.  **Business Critical**
4.  **Virtual Private** is isolated from the global Snowflake program.


## Billing {#billing}

**On demand** and **capacity** models are available. Capacity rewards upfront
payment.

**Costs:**

1.  [Snowflake Credits](https://docs.snowflake.com/en/user-guide/cost-understanding-compute#what-are-credits):
    -   Virtual Warehouse Services
        -   Billed per second, minimum 60 seconds, based on size.
    -   Cloud Services
        -   Metadata operations that don't require a warehouse
        -   Burns 4.4 credits per compute-hour
        -   [Cloud services adjustment](https://docs.snowflake.com/en/user-guide/cost-understanding-compute#understanding-billing-for-cloud-services-usage): only billed if all services exceed
            10% of the daily virtual warehouse credits used.
    -   Serverless Services
        -   Each has its own rate
2.  Dollars &amp; Cents:
    -   [Storage](https://docs.snowflake.com/en/user-guide/cost-understanding-data-storage): tables, stages, time travel data
        -   Charged at a _flat rate per TB_
    -   Data Transfer &amp; Egress
        -   Transfer charges between regions (`COPY INTO`)
        -   Replicating data between regions


# Integration and Connectors {#integration-and-connectors}

**Topics**:

-   Snowsight, Snowpark, Drivers, CLI
-   JDBC, ODBC, Python connector
-   Spark / Snowflake connector
-   Kafka Connector
-   BI tools (Tableau, Power BI, etc.)
-   Data marketplace &amp; data sharing

A variety of methods exist to interact with Snowflake's platform.


## Snowsight {#snowsight}

**Snowsight** is the web interface provided by Snowflake. It is
continuously improved.


## Snowflake Drivers &amp; Connectors {#snowflake-drivers-and-connectors}

**Snowflake Drivers/Connectors** refer to programmatic APIs to interact with
Snowflake from your favourite programming language. The connector for
[python](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector) enables all typical operations, in addition to reading and
writing [pandas dataframes](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-pandas). **Cursors** can be used to connect and execute
SQL statements.


## Snowflake CLI {#snowflake-cli}

**Snowflake CLI** can be installed to connect to Snowflake via the command
line. The legacy client, [snowsql](https://docs.snowflake.com/en/user-guide/snowsql), is now _out of date_.


## Partner Tools {#partner-tools}

**Partner Tools** enable connection to your account via _SSO_ to read and
analyze your data. BI, data integration, security, and governance are
common use cases.


## Snowpark {#snowpark}

**Snowpark** refers to programmatic APIs to run heavy data manipulation
within Snowflake warehouses, leaving the data _within Snowflake_ during
processing.

See the [Snowpark Developer Guide for Python](https://docs.snowflake.com/en/developer-guide/snowpark/python/index). I typically add a
configuration file in `.snowflake/config.toml` with the following
content:

```toml
default_connection_name = "my_main_account"

[connections.my_main_account]
account = "myaccount"
user = "jdoe"
password = "******"
warehouse = "my-wh"
database = "my_db"
schema = "my_schema"

[cli.logs]
save_logs = true
level = "info"
path = "/home/<you>/.snowflake/logs"
```


# Snowflake Objects &amp; DDL Commands {#snowflake-objects-and-ddl-commands}

**Topics:**

-   Databases, Schemas, Tables, Views
-   External Tables, Streams, Tasks
-   Sequences, Stages
-   Examples of CREATE / ALTER / DROP
-   Cloning &amp; object versioning
-   DDL = _data definition language_

**Objects** in Snowflake allow nearly all aspects of the data platform to
be configured with unique access and usage restrictions, from the
**Organization** level down to tables and views.

**Account Objects:**

-   Network Policy
-   User
-   Role
-   Database =&gt; Schema
-   Warehouse
-   Share
-   Resource Monitor

**Schema Objects:**

-   Stage
-   Pipe
-   Procedure
-   Function
-   Table
-   View
-   Task
-   Stream

To work with objects within a schema you must set the **context** for the
following operations with this set of commands:

```sql
USE ROLE <role>;
USE WAREHOUSE <warehouse>;
USE DATABASE <database>;
USE SCHEMA <schema>;
```


## Object Naming Rules {#object-naming-rules}

The name of a database, schema, or table must be unique and start with
'`A-Z`', and are not case sensitive unless encased in double quotes.
Special characters can only be used within quotes.


## General DDL Commands {#general-ddl-commands}

Data Definition Language (DDL) commands are used to manipulate objects
in Snowflake, including setting parameters on account and session objects.

Generally these are available:

-   [USE](https://docs.snowflake.com/en/sql-reference/sql/use) &lt;object&gt;
-   [CREATE](https://docs.snowflake.com/en/sql-reference/sql/create) &lt;object&gt;
-   [ALTER](https://docs.snowflake.com/en/sql-reference/sql/alter) &lt;object&gt;
-   [CREATE OR ALTER](https://docs.snowflake.com/en/sql-reference/sql/create-or-alter) &lt;object&gt;
-   [DROP](https://docs.snowflake.com/en/sql-reference/sql/drop) &lt;object&gt;
-   [SHOW](https://docs.snowflake.com/en/sql-reference/sql/show) &lt;objects&gt;
-   [DESCRIBE](https://docs.snowflake.com/en/sql-reference/sql/desc) &lt;object&gt;
-   [COMMENT](https://docs.snowflake.com/en/sql-reference/sql/comment)

To get the definition for an object, use the following query:

```sql
SELECT get_ddl('<type>', '<NAME>');

-- For example:
SELECT get_ddl('table', 'CUSTOMERS');
```

By default, Snowflake will use all your permissions (`SECONDARY_ROLES
= ALL`) to provide the DDL, revealing all secure features.


## Databases {#databases}

A database is associated with one account. See [docs](https://docs.snowflake.com/en/sql-reference/commands-database#database).

```sql
CREATE DATABASE MY_DATABASE;

-- Cloned
CREATE DATABASE CLONED_DB CLONE MY_DATABASE;

-- Replica
CREATE DATABASE REPLICA_DB AS REPLICA OF MY_DATABASE
  DATA_RETENTION_TIME_IN_DAYS = 3;

-- From share object provided by external account
CREATE DATABASE SHARED_DB FROM SHARE S9DF89.SHARE;

SHOW DATABASES;
```


## Schemas {#schemas}

A schema is associated with one database. See [docs](https://docs.snowflake.com/en/sql-reference/commands-database#schema).

```sql
USE DATABASE MY_DATABASE;
CREATE SCHEMA MY_SCHEMA;

-- Cloned
CREATE SCHEMA CLONED_SCM CLONE MY_DATABASE.MY_SCHEMA;

SHOW SCHEMAS;
SHOW SCHEMAS LIKE 'TPCH%';
```


## Tables {#tables}

A table is associated with one schema. See [docs](https://docs.snowflake.com/en/sql-reference/commands-table#table).

-   Tables are **permanent by default**, but can also be:
    -   Temporary (for just the current Snowsight[^fn:1] session)
    -   Transient (no failsafes)
    -   External (file-based tables stored outside Snowflake)
        -   You can specify the path and type for these.
-   Standard accounts can set **time travel** on permanent tables to a day,
    and enterprise accounts can be set up to 90 days, which enables
    un-dropping the table and restoring from a particular timestamp.

To see if a table is external, and its properties, you can use SHOW:

```sql
SHOW TABLES;
SHOW TABLES LIKE '<table name>';
```


## Views {#views}

-   Views don't contribute to storage cost.
-   Can be used to reveal a subset of table data.
-   **Materialized** views are periodically refreshed and stores the results
    of the query independently from the source table.
-   **Secure** views are only visible to authorized users.
-   Querying views that rely on dropped tables will throw an error.

<!--listend-->

```sql
SHOW VIEWS;
```


# Data Loading &amp; Unloading {#data-loading-and-unloading}

**Topics:**

-   Stages: internal (user, table, named) vs external cloud stages
-   Snowflake Documentation
-   COPY INTO / bulk load
-   Snowpipe (continuous ingestion)
-   Snowflake Documentation
-   Snowpipe Streaming
-   Loading semi-structured data and schema inference


# Querying &amp; Data Manipulation Language {#querying-and-data-manipulation-language}

Data Manipulation Language (DML) refers to the normal SQL methods of
CRUDlike updates to data, with some special Snowflake nuances.

**Topics:**

-   SELECT, INSERT, UPDATE, DELETE, MERGE
-   Working with semi-structured data (VARIANT, OBJECT, ARRAY)
-   Window functions, CTEs, analytic SQL
-   Transforming data in Snowflake


## DECLARE &amp; Snowflake Scripting {#declare-and-snowflake-scripting}

Snowflake SQL has support for procedural logic and error handling
using a built-in SQL extension called [Snowflake scripting](https://docs.snowflake.com/en/developer-guide/snowflake-scripting/index). You can
declare variables and cursors, use control flow logic, handle
exceptions, and update tables.

```sql
DECLARE
  -- Variables, cursors.
BEGIN
  -- SQL statements
EXCEPTION
  -- Error handling
END;
```

This example from the [use cases](https://docs.snowflake.com/en/developer-guide/snowflake-scripting/use-cases#update-table-data-with-user-input) demonstrates variable assignment and a
for loop:

```sql
-- docs.snowflake.com/en/developer-guide/snowflake-scripting/use-cases

CREATE OR REPLACE PROCEDURE apply_bonus(bonus_percentage INT, performance_value INT)
  RETURNS TEXT
  LANGUAGE SQL
AS

DECLARE
  -- Use input to calculate the bonus percentage
  updated_bonus_percentage NUMBER(2,2) DEFAULT (:bonus_percentage/100);
  --  Declare a result set
  rs RESULTSET;

BEGIN
  -- Assign a query to the result set and execute the query
  rs := (SELECT * FROM bonuses);
  -- Use a FOR loop to iterate over the records in the result set
  FOR record IN rs DO
    -- Assign variable values using values in the current record
    LET emp_id_value INT := record.emp_id;
    LET performance_rating_value INT := record.performance_rating;
    LET salary_value NUMBER(12, 2) := record.salary;
    -- Determine whether the performance rating in the record matches the user input
    IF (performance_rating_value = :performance_value) THEN
      -- If the condition is met, update the bonuses table using the calculated bonus percentage
      UPDATE bonuses SET bonus = ( :salary_value * :updated_bonus_percentage )
        WHERE emp_id = :emp_id_value;
    END IF;
  END FOR;
  -- Return text when the stored procedure completes
  RETURN 'Update applied';
END;
```

-   Cursors
-   Resultsets


# Functions &amp; Procedures {#functions-and-procedures}

**Topics:**

-   Stored procedures in SQL, JavaScript, Python
-   UDFs in SQL, JS, Python, Java, and Scala
-   When, why, how, and the results of these calls
-   What is Snowpark[^fn:4], how does it work?


## Stored Procedures (SPs) {#stored-procedures--sps}

**Stored procedures** are named collections of SQL statements. In
Snowflake, these can be created with Snowflake Scripting (SQL) but
also JavaScript and via Snowpark[^fn:4].

SPs cannot be called in SQL statements, but _can_ make use of the
Snowpark API. **The primary goal is to cause side effects in the system.**


### JavaScript Stored Procedures {#javascript-stored-procedures}

There exists a [JavaScript Stored Procedures API](https://docs.snowflake.com/en/developer-guide/stored-procedure/stored-procedures-api) that provides a
`snowflake` object for use within stored procedures written with JS,
enabling branching, looping, error handling, and the dynamic creation
of SQL statements.


## User Defined Functions (UDFs) {#user-defined-functions--udfs}

UDFs are custom functions that can be written in SQL, JS, Python,
Java, or Scala. They can accept parameters and return scalar or
tabular results, can be called from SQL statements, and can be
overloaded. Data is converted to supported types as it is passed to
the functions.

UDFs can be called as part of a SQL statement, returning values for
use, but cannot utilize the Snowpark API or libraries. **The primary
goal is complex data processing.**

```sql
CREATE FUNCTION JS_SQUARE_ROOT(D double)
  RETURNS DOUBLE
  LANGUAGE JAVASCRIPT
  AS
  $$
  return(Math.sqrt(D));
  $$;

-- Call the function
SELECT js_square_root(2);

-- ==> 1.414213562

-- Drop the function
DROP FUNCTION JS_SQUARE_ROOT(DOUBLE);
```

-   UDFs &amp; SPs can call [external network services](https://docs.snowflake.com/en/developer-guide/external-network-access/external-network-access-overview) with configuration.
-   [External functions](https://docs.snowflake.com/en/sql-reference/external-functions-introduction) can be set to POST a REST gateway on
    AWS/GCP/Azure, but they are slower, less secure, cannot be shared,
    and are scalar only.
-   **Overloading:** Functions can have the same name as long as the
    function signature is different.


### Java Functions {#java-functions}

See [Snowflake Docs: Java UDFs](https://docs.snowflake.com/en/developer-guide/udf/java/udf-java-introduction).

**Java** UDFs take `HANDLER` and `TARGET_PATH` parameters - allowing you to
optionally provide a JAR file with classes and functions to use.

```sql
CREATE OR REPLACE FUNCTION inline_hello(name STRING)
RETURNS STRING
LANGUAGE JAVA
AS
$$
    if (name == null) {
        return "Hello, you! Inline function called!";
    }
    return "Hello, " + name + "! Inline function called!";
$$;

-- Call the function:
SELECT inline_hello('Ryan');
```

Or as a pre-compiled JAR:

```java
package com.example;

public class HelloUDF {
    public static String hello(String name) {
        if (name == null) {
            return "Hello, you!";
        }
        return "Hello, " + name + "!";
    }
}
```

Compile to a JAR file:

```bash
javac -d . HelloUDF.java
jar cf hello-udf.jar com/example/HelloUDF.class
```

=&gt; hello-udf.jar

In snowflake, refer to this function in the JAR file:

```sql
CREATE OR REPLACE STAGE my_stage;
PUT file://hello-udf.jar @my_stage auto_compress=false;

-- Define the function
CREATE OR REPLACE FUNCTION hello(name STRING)
  RETURNS STRING
  LANGUAGE JAVA
  IMPORTS = ('@my_stage/hello-udf.jar')
  HANDLER = 'com.example.HelloUDF.hello'
  AS
  $$
    /* Inline Java could be placed here that utilizes functions from the JAR. */
  $$;

-- Call the function:
SELECT hello();
```

==&gt; _[Hello, you!](https://www.youtube.com/watch?v=GO-nsnnmqHA)_


### External Functions {#external-functions}

An `EXTERNAL` function is a lambda or web service behind a proxy.

```sql
CREATE OR REPLACE EXTERNAL FUNCTION blorgon_process(str_input varchar)
  RETURNS variant
  API_INTEGRATION = blorgo9t_08 -- API integration object
  AS 'https://blorgo9t.execute-api.us-west-2.amazonaws.com/prod/blorg'; -- Proxy URL
```


# Warehouses &amp; Compute {#warehouses-and-compute}

**Topics:**

-   What is a warehouse; required for queries / DML
-   Snowflake Documentation
-   Warehouse sizes and billing
-   Snowflake Documentation
-   Auto-suspend, auto-resume
-   Multi-cluster warehouses
-   Best practices for compute sizing &amp; concurrency


## Performance &amp; Cost Optimization {#performance-and-cost-optimization}

**Topics:**

-   Choosing warehouse sizes vs concurrency
-   Pruning / clustering / clustering keys
-   Caching, result caching
-   Query profiling / query history
-   Avoiding overprovisioning / idle compute


# Security &amp; Access Control {#security-and-access-control}

**Topics:**

-   Account access
-   Roles, users, grants, privileges
-   Snowflake Documentation
-   Object-level permissions
-   Row access policies, masking policies
-   Network / IP policies, secure views, encryption

Each **account** is created on specific **provider**, in a particular **region**,
as a single Snowflake **edition**. An **organization** can manage one or more
accounts for different departments, projects, or locations.

The `GLOBALORGADMIN` account can be used to create more accounts and
manage their lifecycle;

```sql
USE ROLE GLOBALORGADMIN;

CREATE ACCOUNT analytics4
  ADMIN_NAME = admin7
  REGION = aws_us_west_2
  etc;

SHOW ACCOUNTS;
```

The short-form of the organization and account is shown in your URL.

```nil
https://uslkjpw-sjl18827.snowflakecomputing.com/
        ------- --------
          Org.    Acct.
```

This URL will prompt you to login, then redirect you to **Snowsight**[^fn:1].


# Advanced Features {#advanced-features}

**Topics:**

-   Time Travel &amp; Fail-safe
-   Zero-copy cloning
-   Materialized Views
-   Tasks &amp; scheduled pipelines
-   Search optimization service
-   Streams &amp; Change Data Capture (CDC)


## Tasks {#tasks}

-   Schedule the execution of a SQL statement or stored procedure.
-

<!--listend-->

```sql
CREATE TASK SYNC1
  WAREHOUSE = WH1
  SCHEDULE = '30 MINUTE' -- can be a CRON expression
AS
  COPY INTO BIG_TABLE_1
  FROM $SOME_STAGE;

-- Start Task Schedule:
-- Requires 'execute task' permission
ALTER TASK SYNC1 RESUME;

-- Stop Task Schedule:
ALTER TASK SYNC1 SUSPEND;
```


### Task Graphs (DAGs) {#task-graphs--dags}

-   A root task can be defined with a schedule
-   Child tasks are defined _without a schedule_, and can have multiple
    parent dependencies to wait for before executing.


## Streams {#streams}

-   Objects created to view and track DML changes to a source table.
-   A stream is queryable, and is created on top of a table.

<!--listend-->

```sql
CREATE STREAM USER_STREAM ON TABLE USERS;
SELECT * FROM USER_STREAM;
```

These have three additional columns:

-   The action (INSERT, UPDATE, DELETE)
-   Whether or not it was a write/update operation
-   The unique Snowflake row ID

Tasks can be configured to process the stream data:

```sql
CREATE TASK NEWUSERS1
  WAREHOUSE = WH1
  SCHEDULE = '5 MINUTE' -- can be a CRON expression
WHEN
  SYSTEM$STREAM_HAS_DATA('USER_STREAM')
AS
  INSERT INTO SEND_EMAIL(ID, NAME) SELECT ID, NAME FROM USER_STREAM;
```


# Tips, Best Practices &amp; Gotchas {#tips-best-practices-and-gotchas}

**Topics:**

-   Avoid leaving warehouses running unnecessarily
-   For small queries, very large warehouses may not help
-   Use clustering only when needed
-   Be aware of billing granularity (per second)
-   Watch out for too many small files in load
-   Query patterns that can disrupt performance


# References &amp; Further Reading {#references-and-further-reading}

Snowflake has tons of interesting documents in their [resource library](https://www.snowflake.com/en/resources/),
including migration advice, using Snowflake as a backing database for
agents, and much more.

**Topics:**

-   Official Snowflake documentation (docs.snowflake.com)
-   Snowflake Documentation
-   Tutorials (“Snowflake in 20 minutes”)
-   Snowflake Documentation
-   Quickstarts &amp; hands-on labs
-   Snowflake Quickstarts
-   SQL command reference
-   Snowflake Documentation

[^fn:1]: **Snowsight** is the [Snowflake web interface](https://docs.snowflake.com/en/user-guide/ui-snowsight).
[^fn:2]: **Columnar Storage** is read-optimized, enabling quick seeking
    through rows without having to read over the entire content of each
    row, like a CSV or other row-oriented storage format.
[^fn:3]: **EC2** or equivalent cloud-based virtual machines.
[^fn:4]: [Snowpark](https://www.snowflake.com/en/product/features/snowpark/) is a multi-language framework for executing remote
    data operations within Snowflake warehouses, close to the data.
